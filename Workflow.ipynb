{
 "metadata": {
  "name": "",
  "signature": "sha256:d3efacb0fd100baf77c201d5a54aa22c5f34b6ddded919f294a0c5de786b42b3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A workflow for machine learning projects"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After a few submissions to <a href=\"http://www.kaggle.com\">Kaggle </a> competitions, (and other small projects), my \"Projects\" folder became unamanageable. After some trial and error, I found a workflow I like, partly borrowed from kaggle master <a href=\"https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4797/starter-code-in-python-with-scikit-learn-auc-885\"> Paul Duan</a>. Hopefully that's useful for someone else!\n",
      "\n",
      "First, the folder structure: \n",
      "\n",
      "    Projects/myproject/\n",
      "        \n",
      "        workflow.py\n",
      "        stacker.py\n",
      "        eda.ipynb\n",
      "        data/\n",
      "            train.csv\n",
      "            test.csv\n",
      "        output/\n",
      "            submissions/\n",
      "                submission1.csv\n",
      "                submission2.csv\n",
      "            pickle/\n",
      "                submission1.pkl\n",
      "                submission2.pkl\n",
      "            models/\n",
      "                submission1.txt\n",
      "                submission2.txt\n",
      "           "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So all the action happens in workflow.py, which looks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SEED = 42  # always use a seed for randomized procedures\n",
      "\n",
      "\n",
      "def load_data(filename, train_set=True):\n",
      "    \"\"\"\n",
      "    Load CSV files. If use_labels, then return the labels of the data set, \n",
      "    for supervised learning. \n",
      "    \"\"\"\n",
      "    \n",
      "    Y_COLUMN = #The labels of the training examples\n",
      "    INDEX_COLUMN = #The id's of the training examples\n",
      "    \n",
      "    df = pd.read_csv(filename)\n",
      "    \n",
      "    if train_set:\n",
      "        labels = df.Y_COLUMN.values\n",
      "  \n",
      "    else:\n",
      "        labels = df.INDEX_COLUMN.values.astype(int)\n",
      "    \n",
      "    df = df.drop('id', axis=1)    \n",
      "  \n",
      "    return labels, data\n",
      "\n",
      "\n",
      "def save_results(idx,predictions, filename):\n",
      "    \"\"\"Given a vector of predictions, save results in CSV format.\"\"\"\n",
      "    submission = pd.DataFrame({\"id\": idx, \"prediction\": predictions})\n",
      "    submission.to_csv(filename, index=False)\n",
      "\n",
      "def main():\n",
      "\n",
      "    # === load data === #\n",
      "    print \"Loading data\"\n",
      "    y, traindata = load_data('./data/train.csv')\n",
      "    idx, testdata = load_data('./data/test.csv', train_set=False)\n",
      "   \n",
      "    ## So this is the real meat: for this example, suppose we have a text classification problem. So we build a pipeline \\\n",
      "    ## to vectorize, reduce dimensionality, rescale and then apply support vector classification. This is wrapped up on a \n",
      "    ## grid search, to get the best parameters.\n",
      "    \n",
      "    \n",
      "    tfv = TfidfVectorizer()\n",
      "    \n",
      "    # Fit TFIDF\n",
      "    tfv.fit(traindata)\n",
      "    X =  tfv.transform(traindata) \n",
      "        \n",
      "    X_sub = tfv.transform(testdata)\n",
      "    \n",
      "    # Initialize SVD\n",
      "    svd = TruncatedSVD()\n",
      "    \n",
      "    # Initialize the standard scaler \n",
      "    scl = StandardScaler()\n",
      "    \n",
      "    # We will use SVM here..\n",
      "    svm_model = SVC()\n",
      "    \n",
      "    # Create the pipeline \n",
      "    clf = pipeline.Pipeline([('svd', svd),\n",
      "    \t\t\t\t\t\t ('scl', scl),\n",
      "                    \t     ('svm', svm_model)])\n",
      "    \n",
      "    # Create a parameter grid to search for best parameters for everything in the pipeline\n",
      "    param_grid = {'svd__n_components' : [300,400],\n",
      "                  'svm__C': [9,10], 'svm__kernel':['linear','rbf'], 'svm__degree':[2,3,4]}\n",
      "    \n",
      "    \n",
      "    # Initialize Grid Search Model\n",
      "    model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid,verbose=10, n_jobs=-1, \n",
      "                                     iid=True, refit=True, cv=5)\n",
      "                                     \n",
      "    # Fit Grid Search Model\n",
      "\n",
      "    model.fit(X,y) # Fit for submission\n",
      "    print(\"Best score: %0.3f\" % model.best_score_)\n",
      "    print(\"Best parameters set:\")\n",
      "    best_parameters = model.best_estimator_.get_params()\n",
      "    for param_name in sorted(param_grid.keys()):\n",
      "    \tprint(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "    \n",
      "    # Get best model\n",
      "    best_model = model.best_estimator_\n",
      "    \n",
      "    # === Predictions === #\n",
      "    # When making predictions, retrain the model on the whole training set\n",
      "    best_model.fit(X, y)\n",
      "    preds = best_model.predict(X_sub)\n",
      "    \n",
      "    \n",
      "    # ===== Save the model and the parameters used ====== #\n",
      "    \n",
      "    filename = raw_input(\"Enter name for submission file: \")\n",
      "    save_results(idx,preds, './output/submissions/'+filename + \".csv\")\n",
      "    joblib.dump(clf,'./output/pickle/'+filename +'.pkl') \n",
      "    with open('./output/models/'+filename+'.txt', 'wb') as f:\n",
      "        f.writelines(str(tfv.get_params)+'\\n')\n",
      "        for key in sorted(param_grid.keys()):\n",
      "            f.writelines(\"\\t%s: %r \\n\" % (param_name, best_parameters[param_name]))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that in the end we save three files: the csv output, the pickled version and a file with the parameters. Pickled versions are rumored to be unstable, so I prefer to save the parameters in case I need to regenerate the model, although I haven't had any problem loading pickled models.\n",
      "\n",
      "Now for the stacker.py file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import math\n",
      "import numpy as np\n",
      "\n",
      "stack_preds = []\n",
      "\n",
      "base_model = pd.read_csv('./output/submissions/sub1.csv')\n",
      "\n",
      "idx = base_model['id'].values.astype(int)\n",
      "\n",
      "for i in range(len(base_model)):\n",
      "    x=int(base_model['prediction'].values[i])\n",
      "    stack_preds.append(x)\n",
      "model_list = ['./output/submissions/sub2.csv','./output/submissions/sub3.csv','./output/submissions/sub4.csv']\n",
      "total_models = len(model_list)+1\n",
      "\n",
      "for model in model_list:\n",
      "    df = pd.read_csv(model)\n",
      "    preds = df['prediction'].values\n",
      "    for i in range(len(preds)):\n",
      "        stack_preds[i] = stack_preds[i] + preds[i]\n",
      "stack_preds = np.array(stack_preds)*1.0/total_models\n",
      "\n",
      "for i in range(len(stack_preds)):\n",
      "    stack_preds[i] =math.floor(stack_preds[i])\n",
      "\n",
      "stack_preds = stack_preds.astype(int)\n",
      "\n",
      "submission = pd.DataFrame({\"id\": idx, \"prediction\": stack_preds})\n",
      "submission.to_csv(\"./output/stacked_subs_1_3.csv\", index=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stacking is a technique for putting different models together. I am not doing anything complicated here, simply averaging the results of my three models. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}